import copy
import logging
import os
import time
from unittest import TestCase

import matplotlib.pyplot as plt
import numpy as np
from ranking_bandit_agent import RankingBanditAgent
import pytest

from ranking_bandit_agent import RankingBanditAgent
from simulator import AbstractSimulator, DatasetSimulator, GenerativeSimulator

LOG: logging.Logger = logging.getLogger(__name__)


PLOTS_SAVE_PATH = "./sim_plots"
RESULTS_SAVE_PATH = "./sim_results"


CONTINUOUS_CONTEXT_COLUMNS = [
    "context__1",
    "context__2",
    "context__3",
    "context__4",
    "context__5",
    "context__6",
    "context__7",
]

TRAINING_DATA_CSV = "training_data.csv"


class RankingBanditAgentSimulationTests(TestCase):
    def setUp(self) -> None:
        self.n_items = 3

        self.ranking_agent = RankingBanditAgent(
            state_columns=CONTINUOUS_CONTEXT_COLUMNS,
            reward_col="reward__ordered",
            top_k=self.n_items,
            randomization_horizon=5,
        )

    def test_regret(
        self,
        runs: int = 25,
        steps: int = 150,
        batch_size: int = 1,
        use_all_data: bool = False,
        reward_noise: float = 0.0,
    ) -> None:
        """Run a simulation with the ranking agent and compare the total regret to that of the optimal policy
        where the optimal policy is computed using an 'oracle' that knows the true reward distribution. The 'true'
        reward distribution is generated by pretraining a Logistic Regression model on the provided historical data.

        Parameters
        ----------
            runs: int
                The simulations to perform per ksi value (UCB parameter)
            steps: int
                The number of steps to run each simulation for.
            use_all_data: bool
                Whether to use the cumulatively observed data for each update of the model, or to only use the data
                observed in the current batch
            reward_noise: float
                The amount of noise to add to the reward. Modelled as adding a random number from a normal distribution
                with mean 0 and standard deviation reward_noise to the bernoulli parameter of true reward distribution.
        """

        agent = self.ranking_agent

        # ensure the save paths exist
        os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)
        os.makedirs(PLOTS_SAVE_PATH, exist_ok=True)

        # config = self.ranking_agent_config
        agent.objective = "Profit"
        agent.action_weight_col = "sale_price"
        ksi_list = [0, 0.25, 0.5, 1, 2]

        regret: dict[float, np.ndarray] = {}
        cumulative: dict[float, np.ndarray] = {}
        for ksi in ksi_list:
            all_runs_regret = []
            agent.ksi = ksi

            ranking_agent = copy.deepcopy(agent)  # type: ignore
            ground_truth_agent = copy.deepcopy(agent)
            sim = GenerativeSimulator(ranking_agent, ground_truth_agent, self.n_items)

            print(f"Starting {runs} runs with ksi={ksi}")
            tik = time.perf_counter()
            for _ in range(runs):
                sample_regret = sim.evaluate(
                    steps=steps,
                    batch_size=batch_size,
                    add_noise=reward_noise,
                    use_all_data=use_all_data,
                    data_path=None,
                )
                all_runs_regret.append(sample_regret)

            ksi_regret = np.array(all_runs_regret)
            cumulative_regret = np.cumsum(ksi_regret, axis=1)
            run_time = time.perf_counter() - tik
            run_time_min, run_time_sec = divmod(run_time, 60)
            run_tim_hrs, run_time_min = divmod(run_time_min, 60)
            print(
                f"Finished {runs} runs with an average cumulative regret of {np.mean(cumulative_regret, axis=0)[-1]:0.4f} "
                f"(Total Run Time = {run_tim_hrs}h {run_time_min:0.0f}m {run_time_sec:0.0f}s)"
            )

            np.save(
                f"./sim_results/simulation_results_items_{self.n_items}_runs_{runs}_steps_{steps}_"
                + f"rh_{agent.randomization_horizon}_ksi_{ksi}_bnorm_{0.2}_noise_{reward_noise}",
                ksi_regret,
            )
            regret[ksi] = np.mean(all_runs_regret, axis=0)
            cumulative[ksi] = np.mean(cumulative_regret, axis=0)

        # Generate Regret Plots
        fig, axes = plt.subplots(ncols=2, figsize=(20, 8))
        regret_ax, cumulative_regret_ax = axes

        cmap = plt.cm.coolwarm(np.linspace(0, 1, len(ksi_list)))
        for color, (ksi, ksi_regret) in zip(cmap, regret.items()):
            label = "Agent Regret" + (f"(ksi={ksi})" if 0 < ksi else "(MLE)")
            regret_ax.plot(range(ksi_regret.shape[0]), ksi_regret, label=label, color=color)
            cumulative_regret_ax.plot(range(ksi_regret.shape[0]), cumulative[ksi], label=label, color=color)

        fig.suptitle(f"Simulation Results", fontsize=26)
        cumulative_regret_ax.set_title(f"Simulation Results", fontsize=22)

        for ax in axes:
            ax.set_ylabel(f"Average Regret (Over {runs} Runs)", fontsize=16)
            ax.set_xlabel("Time Steps", fontsize=16)
            ax.grid(axis="y", alpha=0.4)
            ax.legend()

        fig.savefig(
            f"./sim_plots/simulation_plot_items_{self.n_items}_runs_{runs}_steps_{steps}.png"  # noqa
        )
        plt.clf()
