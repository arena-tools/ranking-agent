import logging
import os
import copy
import time
from itertools import chain
from typing import List, Type
from unittest import TestCase

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytest

from ranking_bandit_agent import Profit
# from feature_quantizer import CategoricalFeature, FeatureOneHotQuantizer

from ranking_bandit_agent import PROPENSITY_COL, UCB_PROPENSITY_COL, UNCERTAINTY_COL, RankingBanditAgent
from simulator import AbstractSimulator, GenerativeSimulator

LOG: logging.Logger = logging.getLogger(__name__)


PLOTS_SAVE_PATH = "./sim_plots"
RESULTS_SAVE_PATH = "./sim_results"

MEASURED_REWARD_COLUMN = "realized_reward"
STATE_COLUMNS: List[str] = ["state_0", "state_1", "state_2"]
ACTION_ID: str = "test_action_id"
CONTEXT_COLUMN_PREFIX = "context__"
EXTRA_COLUMN_PREFIX = "extra__"
QUANTIZER_SUFFIX = "__quantized"
USER_ID_COLUMN = "user_id"
PRODUCT_ID_COLUMN = "product_id"
RANK_COLUMN = "rank_col"


class RankingBanditAgentSimulationTests(TestCase):
    def setUp(self) -> None:
        self.n_items = 5
        self.context_dim = 7

        self.agent = RankingBanditAgent(
            context_dim=self.context_dim,
            n_items=self.n_items,
            randomization_horizon=5,
        )

    
    def test_predict(self) -> None:
        # The example here is that given we are out of coke,
        # In what order should we reccommend the 5 beverages?

        for i in range(10):
            input_df = np.random.rand(self.n_items, i, self.context_dim+1) # first dim for rank
            clicks = np.random.choice([0, 1], p=[0.5, 0.5], size=(self.n_items,1))
            self.agent.update(input_df, clicks)

        # breakpoint()
        output_df = self.agent.rank(input_df[:, :, 1:])

        # # Modify the below expected_output_df here
        # expected_output_df = pd.DataFrame(
        #     {
        #         "user_id": ["123"] * 5,
        #         "id__context": ["coke"] * 5,
        #         "id__action__context": ["pepsi", "sprite", "juice", "coffee", "tea"],
        #         "p_value": [0.5] * 5,
        #     }
        # )
        # pd.testing.assert_frame_equal(output_df, expected_output_df)

    def test_fit_and_update(self) -> None:
        input_df = pd.DataFrame(
            {
                "user_id": ["123", "123", "123", "123", "123"],
                "id__context": ["coke"] * 5,
                "context__price": [100, 100, 100, 100, 100],
                "action__context__price": [150, 100, 120, 130, 140],
                "action__rank_position": [0, 1, 2, 3, 4],
                "reward__ordered": [0, 1, 1, 1, 0],
            }
        )
        for i in range(100):
            print("iter ", i)
            self.ranking_agent.update(input_df)
            ranking = self.ranking_agent.predict()
            input_df["action__rank_position"] = ranking
            if np.array(ranking.values)[0] == 1 and np.array(ranking.values)[1] == 0:
                input_df["reward__ordered"] = [1, 1, 0, 0, 0]
            else:
                input_df["reward__ordered"] = list(np.random.choice([0, 1], size=(5,), p=[14.0 / 15, 1.0 / 15]))
                # input_df["reward"] = [0, 0, 0, 0, 0]

            # print("params: ", self.ranking_agent.params)
            print("ranking: ", ranking.ranking.tolist())
            print()

        pd.testing.assert_frame_equal(ranking[:2], pd.DataFrame([1, 0], columns=["ranking"]))


    @pytest.mark.agent_performance_slow_test
    def test_regret_generative(
        self,
        runs: int = 100,
        steps: int = 100,
        batch_size: int = 1,
        use_all_data: bool = True,
        reward_noise: float = 0.0,
    ) -> None:
        """Run a simulation with the ranking agent and compare the total regret to that of the optimal policy
        where the optimal policy is computed using an 'oracle' that knows the true reward distribution. The 'true'
        reward distribution is generated by pretraining a Logistic Regression model on the provided historical data.

        Parameters
        ----------
            runs: int
                The simulations to perform per ksi value (UCB parameter)
            steps: int
                The number of steps to run each simulation for.
            use_all_data: bool
                Whether to use the cumulatively observed data for each update of the model, or to only use the data
                observed in the current batch
            reward_noise: float
                The amount of noise to add to the reward. Modelled as adding a random number from a normal distribution
                with mean 0 and standard deviation reward_noise to the bernoulli parameter of true reward distribution.
            source_file: str
                The path to the csv file containing the historical data to use for pretraining the model.
        """
        self._test_regret_base(
            simulator=GenerativeSimulator,
            runs=runs,
            steps=steps,
            batch_size=batch_size,
            use_all_data=use_all_data,
            reward_noise=reward_noise,
        )
    
    def _test_regret_base(
        self,
        simulator: Type[AbstractSimulator],
        runs: int = 100,
        steps: int = 100,
        batch_size: int = 1,
        use_all_data: bool = True,
        reward_noise: float = 0.0,
    ) -> None:
        """Run a simulation with the ranking agent and compare the total regret to that of the optimal policy
        where the optimal policy is computed using an 'oracle' that knows the true reward distribution. The 'true'
        reward distribution is generated by pretraining a Logistic Regression model on the provided historical data.

        Parameters
        ----------
            runs: int
                The simulations to perform per ksi value (UCB parameter)
            steps: int
                The number of steps to run each simulation for.
            use_all_data: bool
                Whether to use the cumulatively observed data for each update of the model, or to only use the data
                observed in the current batch
            reward_noise: float
                The amount of noise to add to the reward. Modelled as adding a random number from a normal distribution
                with mean 0 and standard deviation reward_noise to the bernoulli parameter of true reward distribution.
            source_file: str
                The path to the csv file containing the historical data to use for pretraining the model.
        """

        # ensure the save paths exist
        os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)
        os.makedirs(PLOTS_SAVE_PATH, exist_ok=True)

        ksi_list = [0, 0.03, 0.05, 0.09, 0.1]

        regret: dict[float, np.ndarray] = {}
        cumulative: dict[float, np.ndarray] = {}
        for ksi in ksi_list:
            all_runs_regret = []

            ranking_agent = copy.deepcopy(self.agent)  # type: ignore
            ground_truth_agent = copy.deepcopy(self.agent)
            ranking_agent.ksi = ksi
            ground_truth_agent.ksi = ksi
            sim = simulator(ranking_agent, ground_truth_agent, self.n_items)

            print(f"Starting {runs} runs with ksi={ksi}")
            tik = time.perf_counter()
            for _ in range(runs):
                sample_regret = sim.evaluate(
                    steps=steps,
                    batch_size=batch_size,
                    add_noise=reward_noise,
                    use_all_data=use_all_data,
                )
                all_runs_regret.append(sample_regret)

            ksi_regret = np.array(all_runs_regret)
            cumulative_regret = np.cumsum(ksi_regret, axis=1)
            run_time = time.perf_counter() - tik
            run_time_min, run_time_sec = divmod(run_time, 60)
            run_tim_hrs, run_time_min = divmod(run_time_min, 60)
            print(
                f"Finished {runs} runs with an average cumulative regret of {np.mean(cumulative_regret, axis=0)[-1]:0.4f} "
                f"(Total Run Time = {run_tim_hrs}h {run_time_min:0.0f}m {run_time_sec:0.0f}s)"
            )

            np.save(
                f"./sim_results/simulation_cat_results_items_{self.n_items}_runs_{runs}_steps_{steps}_"
                + f"rh_{ranking_agent.randomization_horizon}_ksi_{ksi}_bnorm_{0.2}_noise_{reward_noise}",
                ksi_regret,
            )
            regret[ksi] = np.mean(all_runs_regret, axis=0)
            cumulative[ksi] = np.mean(cumulative_regret, axis=0)

        # Generate Regret Plots
        fig, axes = plt.subplots(ncols=2, figsize=(20, 8))
        regret_ax, cumulative_regret_ax = axes

        cmap = plt.cm.coolwarm(np.linspace(0, 1, len(ksi_list)))
        for color, (ksi, ksi_regret) in zip(cmap, regret.items()):
            label = "Agent Regret" + (f"(ksi={ksi})" if 0 < ksi else "(MLE)")
            regret_ax.plot(range(ksi_regret.shape[0]), ksi_regret, label=label, color=color)
            cumulative_regret_ax.plot(range(ksi_regret.shape[0]), cumulative[ksi], label=label, color=color)

        fig.suptitle(f"Simulation Results", fontsize=26)
        cumulative_regret_ax.set_title(f"Simulation Results", fontsize=22)

        for ax in axes:
            ax.set_ylabel(f"Average Regret (Over {runs} Runs)", fontsize=16)
            ax.set_xlabel("Time Steps", fontsize=16)
            ax.grid(axis="y", alpha=0.4)
            ax.legend()

        fig.savefig(
            f"./sim_plots/simulation_cat_plot_items_{self.n_items}_runs_{runs}_steps_{steps}.png"  # noqa
        )
        plt.clf()
