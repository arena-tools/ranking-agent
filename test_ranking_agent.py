import logging
import os
import copy
import time
from typing import Type
from unittest import TestCase

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytest

from ranking_bandit_agent import RankingBanditAgent
from simulator import AbstractSimulator, GenerativeSimulator

LOG: logging.Logger = logging.getLogger(__name__)


PLOTS_SAVE_PATH = "./sim_plots"
RESULTS_SAVE_PATH = "./sim_results"


class RankingBanditAgentSimulationTests(TestCase):
    def setUp(self) -> None:
        self.n_items = 10
        self.k = 5
        self.context_dim = 7

        self.agent = RankingBanditAgent(
            context_dim=self.context_dim,
            n_items=self.n_items,
            select_items=self.k,
            randomization_horizon=5,
        )


    def test_regret_generative(
        self,
        runs: int = 300,
        steps: int = 500,
        batch_size: int = 1,
        use_all_data: bool = True,
        reward_noise: float = 0.0,
    ) -> None:
        """Run a simulation with the ranking agent and compare the total regret to that of the optimal policy
        where the optimal policy is computed using an 'oracle' that knows the true reward distribution. The 'true'
        reward distribution is generated by pretraining a Logistic Regression model on the provided historical data.

        Parameters
        ----------
            runs: int
                The simulations to perform per ksi value (UCB parameter)
            steps: int
                The number of steps to run each simulation for.
            use_all_data: bool
                Whether to use the cumulatively observed data for each update of the model, or to only use the data
                observed in the current batch
            reward_noise: float
                The amount of noise to add to the reward. Modelled as adding a random number from a normal distribution
                with mean 0 and standard deviation reward_noise to the bernoulli parameter of true reward distribution.
            source_file: str
                The path to the csv file containing the historical data to use for pretraining the model.
        """
        self._test_regret_base(
            simulator=GenerativeSimulator,
            runs=runs,
            steps=steps,
            batch_size=batch_size,
            use_all_data=use_all_data,
            reward_noise=reward_noise,
        )
    
    def _test_regret_base(
        self,
        simulator: Type[AbstractSimulator],
        runs: int = 300,
        steps: int = 500,
        batch_size: int = 1,
        use_all_data: bool = True,
        reward_noise: float = 0.0,
    ) -> None:
        """Run a simulation with the ranking agent and compare the total regret to that of the optimal policy
        where the optimal policy is computed using an 'oracle' that knows the true reward distribution. The 'true'
        reward distribution is generated by pretraining a Logistic Regression model on the provided historical data.

        Parameters
        ----------
            runs: int
                The simulations to perform per ksi value (UCB parameter)
            steps: int
                The number of steps to run each simulation for.
            use_all_data: bool
                Whether to use the cumulatively observed data for each update of the model, or to only use the data
                observed in the current batch
            reward_noise: float
                The amount of noise to add to the reward. Modelled as adding a random number from a normal distribution
                with mean 0 and standard deviation reward_noise to the bernoulli parameter of true reward distribution.
            source_file: str
                The path to the csv file containing the historical data to use for pretraining the model.
        """

        # ensure the save paths exist
        os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)
        os.makedirs(PLOTS_SAVE_PATH, exist_ok=True)

        ksi_list = [0, 0.1, 0.3, 0.7, 1.0]

        regret: dict[float, np.ndarray] = {}
        cumulative: dict[float, np.ndarray] = {}
        for ksi in ksi_list:
            all_runs_regret = []

            ranking_agent = copy.deepcopy(self.agent)  # type: ignore
            ground_truth_agent = copy.deepcopy(self.agent)
            ranking_agent.ksi = ksi
            ground_truth_agent.ksi = ksi
            sim = simulator(ranking_agent, ground_truth_agent, self.n_items, k=self.k)

            print(f"Starting {runs} runs with ksi={ksi}")
            tik = time.perf_counter()
            for _ in range(runs):
                sample_regret = sim.evaluate(
                    steps=steps,
                    batch_size=batch_size,
                    add_noise=reward_noise,
                    use_all_data=use_all_data,
                )
                all_runs_regret.append(sample_regret)
                print(
                    f"Finished runs {_}"
                )

            ksi_regret = np.array(all_runs_regret)
            cumulative_regret = np.cumsum(ksi_regret, axis=1)
            run_time = time.perf_counter() - tik
            run_time_min, run_time_sec = divmod(run_time, 60)
            run_tim_hrs, run_time_min = divmod(run_time_min, 60)
            print(
                f"Finished {runs} runs with an average cumulative regret of {np.mean(cumulative_regret, axis=0)[-1]:0.4f} "
                f"(Total Run Time = {run_tim_hrs}h {run_time_min:0.0f}m {run_time_sec:0.0f}s)"
            )

            np.save(
                f"./sim_results/simulation_cat_results_items_{self.n_items}_runs_{runs}_steps_{steps}_"
                + f"rh_{ranking_agent.randomization_horizon}_ksi_{ksi}_bnorm_{0.2}_noise_{reward_noise}",
                ksi_regret,
            )
            regret[ksi] = np.mean(all_runs_regret, axis=0)
            cumulative[ksi] = np.mean(cumulative_regret, axis=0)

        # Generate Regret Plots
        fig, axes = plt.subplots(ncols=2, figsize=(20, 8))
        regret_ax, cumulative_regret_ax = axes

        cmap = plt.cm.coolwarm(np.linspace(0, 1, len(ksi_list)))
        for color, (ksi, ksi_regret) in zip(cmap, regret.items()):
            label = "Agent Regret" + (f"(ksi={ksi})" if 0 < ksi else "(MLE)")
            regret_ax.plot(range(ksi_regret.shape[0]), ksi_regret, label=label, color=color)
            cumulative_regret_ax.plot(range(ksi_regret.shape[0]), cumulative[ksi], label=label, color=color)

        fig.suptitle(f"Simulation Results", fontsize=26)

        plt_num = 0
        for ax in axes:
            if plt_num == 0:
                ax.set_ylabel(f"Average Regret (Over {runs} Runs)", fontsize=16)
            else:
                ax.set_ylabel(f"Cumulative Average Regret (Over {runs} Runs)", fontsize=16)
            ax.set_xlabel("Time Steps", fontsize=16)
            ax.grid(axis="y", alpha=0.4)
            ax.legend()

            plt_num += 1

        fig.savefig(
            f"./sim_plots/simulation_cat_plot_items_{self.n_items}_runs_{runs}_steps_{steps}.png"  # noqa
        )
        plt.clf()
